"""
app.py â€” Minimal Streamlit app to chat over your ingested PDFs/web pages.

A. Purpose
-------
Provide a friendly chat UI with:
- Question box + Ask button,
- Summary/Detailed toggle,
- Source snippets expander,
- Stateful chat history per session.

B. How it works
------------
1) On startup: load settings, vector store, retriever, and LLM.
2) User enters a question.
3) We prepend a small style hint (Summary vs Detailed) to the question.
4) We pass (question + history) to the ConversationalRetrievalChain.
5) We render the answer and show source chunks.

C. Run
---
streamlit run app.py
"""

import streamlit as st
from typing import List, Tuple

from chat_core import (
    get_settings,
    load_vectorstore,
    build_retriever,
    make_llm,
    build_chain,
    answer_question,
)

# -----------------
# 0) Page settings
# -----------------
st.set_page_config(page_title="Ask Your PDFs", page_icon="ðŸ“š", layout="wide")
st.title("ðŸ“š Ask Your PDFs & Web Pages")

# -----------------------
# 1) Initialize everything
# -----------------------

# Saves heavy resources across reruns for streamlit apps
@st.cache_resource(show_spinner=False)
# It stores heavy resources to speed up reruns. On rerun, the cached objects are returned instantly if the inputs/config didnâ€™t change
def bootstrap():
    """
    Cache heavy resources across reruns:
    - vector store + retriever
    - LLM and chain
    """
    # Here cfg and chain are local variables, but we return them to be stored as cached resources
    cfg = get_settings()
    vs = load_vectorstore(cfg["db_dir"], cfg["embed_model"])
    # Builds the retriever object from the vector store with top-k setting
    retriever = build_retriever(vs, cfg["top_k"])
    llm = make_llm(
        provider=cfg["llm_provider"],
        temperature=cfg["temperature"],
        openai_model=cfg["openai_model"],
        ollama_model=cfg["ollama_model"],
    )
    chain = build_chain(retriever, llm)
    return cfg, chain

# cfg and chain are kind of global for the app lifetime and stores bootstrap value across reruns
cfg, chain = bootstrap()

# ---------------------------------
# 2) Session-scoped chat transcript
# ---------------------------------

# Checks if history exists in session state, else initializes it to retain the user's context across interactions
if "history" not in st.session_state:
    # store as list of (user_text, ai_text)
    st.session_state.history: List[Tuple[str, str]] = []

# ---------------------
# 3) Controls + inputs
# ---------------------
with st.sidebar:
    st.header("Settings")
    st.write(f"LLM provider: **{cfg['llm_provider']}**")
    st.write(f"Retriever k: **{cfg['top_k']}**")
    st.caption(f"DB: `{cfg['db_dir']}` Â· Embeddings: `{cfg['embed_model']}`")

style = st.radio("Response style", ["Summary", "Detailed"], horizontal=True)
question = st.text_input("Ask a question about your documents:")

# ---------------
# 4) Ask + render
# ---------------
if st.button("Ask") and question.strip():
    # Build a light style hint to bias the LLM without over-engineering prompts.
    style_hint = (
        "Give a brief, 3â€“5 sentence summary."
        if style == "Summary"
        else "Provide a detailed, step-by-step explanation with brief quotes from sources when useful."
    )
    styled_question = f"[User prefers: {style}] {style_hint}\n\nQuestion: {question.strip()}"

    # Convert chat history to required (human, ai) tuples.Conversion of lc_history not needed as _coerce_history is called inside answer_question   
    # lc_history = [(u, a) for (u, a) in st.session_state.history]

    with st.spinner("Thinking..."):
        # LCEL chain in chat_core returns {"answer": str, "sources": list[Document]}
        result = answer_question(chain, styled_question, st.session_state.history)  # -- no need to convert history here again to lc_history

    # Append to session history
    st.session_state.history.append((question.strip(), result["answer"]))

    # Render answer
    st.markdown("### Answer")
    st.write(result["answer"])

    # Render sources (Documents from retriever)
    sources = result["sources"]
    if sources:
        with st.expander("Show cited source chunks"):
            for i, d in enumerate(sources, 1):
                meta = d.metadata
                source = meta.get("source", "unknown")
                page = meta.get("page", "n/a")
                st.markdown(f"**{i}.** {source}  (p. {page})")
                st.code((d.page_content or "")[:800] + ("â€¦" if len(d.page_content) > 800 else ""))

# ---------------
# 5) Chat transcript
# ---------------
if st.session_state.history:
    st.markdown("---")
    st.subheader("Chat Transcript")
    for i, (u, a) in enumerate(st.session_state.history, 1):
        st.markdown(f"**You:** {u}")
        st.markdown(f"**AI:** {a}")
        if i != len(st.session_state.history):
            st.markdown("---")
